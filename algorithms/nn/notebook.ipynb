{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 神经网络实现详解\n",
    "\n",
    "本笔记本包含神经网络的完整实现和可视化实验。\n",
    "\n",
    "## 目录\n",
    "1. [从零实现神经网络](#从零实现神经网络)\n",
    "2. [激活函数详解](#激活函数详解)\n",
    "3. [反向传播算法](#反向传播算法)\n",
    "4. [梯度消失问题](#梯度消失问题)\n",
    "5. [深度学习框架](#深度学习框架)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 从零实现神经网络\n",
    "\n",
    "首先，我们从零开始实现一个简单的全连接神经网络。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification, make_circles\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class NeuralNetwork:\n",
    "    \"\"\"简单神经网络实现\"\"\"\n",
    "    \n",
    "    def __init__(self, layer_sizes, activation='relu'):\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.activation = activation\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        \n",
    "        # 初始化参数\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            # Xavier 初始化\n",
    "            limit = np.sqrt(6 / (layer_sizes[i] + layer_sizes[i + 1]))\n",
    "            w = np.random.uniform(-limit, limit,\n",
    "                              (layer_sizes[i], layer_sizes[i + 1]))\n",
    "            b = np.zeros((1, layer_sizes[i + 1]))\n",
    "            self.weights.append(w)\n",
    "            self.biases.append(b)\n",
    "    \n",
    "    def _activate(self, z, derivative=False):\n",
    "        \"\"\"激活函数\"\"\"\n",
    "        if self.activation == 'relu':\n",
    "            if derivative:\n",
    "                return (z > 0).astype(float)\n",
    "            return np.maximum(0, z)\n",
    "        elif self.activation == 'sigmoid':\n",
    "            if derivative:\n",
    "                s = 1 / (1 + np.exp(-z))\n",
    "                return s * (1 - s)\n",
    "            return 1 / (1 + np.exp(-z))\n",
    "        elif self.activation == 'tanh':\n",
    "            if derivative:\n",
    "                return 1 - np.tanh(z)**2\n",
    "            return np.tanh(z)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"前向传播\"\"\"\n",
    "        self.a = [X]\n",
    "        self.z = []\n",
    "        \n",
    "        for i in range(len(self.weights)):\n",
    "            z = np.dot(self.a[-1], self.weights[i]) + self.biases[i]\n",
    "            self.z.append(z)\n",
    "            a = self._activate(z)\n",
    "            self.a.append(a)\n",
    "        \n",
    "        return self.a[-1]\n",
    "    \n",
    "    def backward(self, X, y, learning_rate=0.01):\n",
    "        \"\"\"反向传播\"\"\"\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        # 前向传播\n",
    "        output = self.forward(X)\n",
    "        \n",
    "        # 输出层梯度\n",
    "        delta = output - y\n",
    "        dW = []\n",
    "        db = []\n",
    "        \n",
    "        # 反向传播\n",
    "        for i in range(len(self.weights) - 1, -1, -1):\n",
    "            dW.insert(0, np.dot(self.a[i].T, delta) / m)\n",
    "            db.insert(0, np.sum(delta, axis=0, keepdims=True) / m)\n",
    "            \n",
    "            if i > 0:\n",
    "                delta = np.dot(delta, self.weights[i].T) * \\\n",
    "                       self._activate(self.z[i-1], derivative=True)\n",
    "        \n",
    "        # 更新参数\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] -= learning_rate * dW[i]\n",
    "            self.biases[i] -= learning_rate * db[i]\n",
    "    \n",
    "    def train(self, X, y, epochs=1000, learning_rate=0.01, verbose=True):\n",
    "        \"\"\"训练网络\"\"\"\n",
    "        losses = []\n",
    "        for epoch in range(epochs):\n",
    "            self.backward(X, y, learning_rate)\n",
    "            \n",
    "            if epoch % 100 == 0:\n",
    "                output = self.forward(X)\n",
    "                loss = np.mean(np.square(output - y))\n",
    "                losses.append(loss)\n",
    "                if verbose:\n",
    "                    print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "        \n",
    "        return losses\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"预测\"\"\"\n",
    "        return (self.forward(X) > 0.5).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试网络 - XOR 问题\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "# 创建网络\n",
    "nn = NeuralNetwork([2, 4, 1], activation='sigmoid')\n",
    "\n",
    "# 训练\n",
    "losses = nn.train(X, y, epochs=5000, learning_rate=0.1)\n",
    "\n",
    "# 测试\n",
    "print(\"\\n预测结果:\")\n",
    "for i in range(len(X)):\n",
    "    pred = nn.predict(X[i:i+1])[0][0]\n",
    "    print(f\"输入: {X[i]}, 预测: {pred}, 真实: {y[i][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 激活函数详解\n",
    "\n",
    "让我们对比不同激活函数的特性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化激活函数\n",
    "x = np.linspace(-5, 5, 100)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# ReLU\n",
    "relu = np.maximum(0, x)\n",
    "axes[0].plot(x, relu, linewidth=2)\n",
    "axes[0].set_title('ReLU')\n",
    "axes[0].set_xlabel('x')\n",
    "axes[0].set_ylabel('f(x)')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Sigmoid\n",
    "sigmoid = 1 / (1 + np.exp(-x))\n",
    "axes[1].plot(x, sigmoid, linewidth=2)\n",
    "axes[1].set_title('Sigmoid')\n",
    "axes[1].set_xlabel('x')\n",
    "axes[1].set_ylabel('f(x)')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Tanh\n",
    "tanh = np.tanh(x)\n",
    "axes[2].plot(x, tanh, linewidth=2)\n",
    "axes[2].set_title('Tanh')\n",
    "axes[2].set_xlabel('x')\n",
    "axes[2].set_ylabel('f(x)')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 反向传播算法\n",
    "\n",
    "反向传播是神经网络训练的核心算法。让我们详细理解梯度如何向后流动。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_backprop():\n",
    "    \"\"\"可视化反向传播过程\"\"\"\n",
    "    # 创建简单网络\n",
    "    nn = NeuralNetwork([2, 3, 1], activation='sigmoid')\n",
    "    \n",
    "    # 单个样本\n",
    "    X_sample = np.array([[0.5, 0.5]])\n",
    "    y_sample = np.array([[1]])\n",
    "    \n",
    "    # 前向传播\n",
    "    output = nn.forward(X_sample)\n",
    "    loss = np.mean((output - y_sample) ** 2)\n",
    "    \n",
    "    print(\"=== 前向传播 ===\")\n",
    "    print(f\"输入: {X_sample[0]}\")\n",
    "    for i, a in enumerate(nn.a[1:-1]):\n",
    "        print(f\"隐藏层 {i+1} 激活: {a[0]}\")\n",
    "    print(f\"输出: {output[0][0]:.4f}\")\n",
    "    print(f\"损失: {loss:.4f}\")\n",
    "    \n",
    "    print(\"\\n=== 反向传播 ===\")\n",
    "    # 计算输出层梯度\n",
    "    delta_output = (output - y_sample) * output * (1 - output)\n",
    "    print(f\"输出层 delta: {delta_output[0]}\")\n",
    "    \n",
    "    # 反向传播\n",
    "    for i in range(len(nn.weights) - 1, -1, -1):\n",
    "        if i > 0:\n",
    "            delta_hidden = np.dot(delta_output, nn.weights[i].T) * \\\n",
    "                          nn.a[i] * (1 - nn.a[i])\n",
    "            print(f\"隐藏层 {i} delta: {delta_hidden[0]}\")\n",
    "            delta_output = delta_hidden\n",
    "\n",
    "visualize_backprop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度消失问题\n",
    "\n",
    "深层网络中，梯度可能会在反向传播过程中逐渐消失。让我们演示这个问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_vanishing_gradient():\n",
    "    \"\"\"演示梯度消失\"\"\"\n",
    "    depths = range(2, 11)\n",
    "    sigmoid_grads = []\n",
    "    relu_grads = []\n",
    "    \n",
    "    x = 1.0  # 输入值\n",
    "    \n",
    "    for depth in depths:\n",
    "        # Sigmoid 梯度\n",
    "        sigmoid_grad = 1\n",
    "        for _ in range(depth):\n",
    "            sigmoid_grad *= (1 / (1 + np.exp(-x))) * (1 - 1 / (1 + np.exp(-x)))\n",
    "        sigmoid_grads.append(sigmoid_grad)\n",
    "        \n",
    "        # ReLU 梯度\n",
    "        relu_grad = 1\n",
    "        for _ in range(depth):\n",
    "            relu_grad *= 1 if x > 0 else 0\n",
    "        relu_grads.append(relu_grad)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.semilogy(depths, sigmoid_grads, 'o-', label='Sigmoid')\n",
    "    plt.semilogy(depths, relu_grads, 's-', label='ReLU')\n",
    "    plt.xlabel('网络深度')\n",
    "    plt.ylabel('梯度值（对数坐标）')\n",
    "    plt.title('梯度消失问题演示')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Sigmoid 梯度随深度变化:\")\n",
    "    for d, grad in zip(depths, sigmoid_grads):\n",
    "        print(f\"深度 {d}: {grad:.6e}\")\n",
    "\n",
    "demonstrate_vanishing_gradient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 深度学习框架\n",
    "\n",
    "实际应用中，我们使用 TensorFlow 或 PyTorch。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow 示例\n",
    "import tensorflow as tf\n",
    "from sklearn.datasets import make_circles\n",
    "\n",
    "# 生成数据\n",
    "X, y = make_circles(n_samples=500, noise=0.1, factor=0.5, random_state=42)\n",
    "y = y.reshape(-1, 1)\n",
    "\n",
    "# 构建模型\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(16, activation='relu', input_shape=(2,)),\n",
    "    tf.keras.layers.Dense(8, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 训练\n",
    "history = model.fit(X, y, epochs=100, batch_size=32, verbose=0)\n",
    "\n",
    "# 评估\n",
    "loss, accuracy = model.evaluate(X, y, verbose=0)\n",
    "print(f\"训练集准确率: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 逐层激活可视化\n",
    "\n",
    "让我们可视化网络各层的激活模式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_layer_activations(model, X, y):\n",
    "    \"\"\"可视化各层激活\"\"\"\n",
    "    # 创建中间层模型\n",
    "    layer_outputs = [layer.output for layer in model.layers]\n",
    "    activation_model = tf.keras.Model(inputs=model.input, outputs=layer_outputs)\n",
    "    \n",
    "    # 获取激活\n",
    "    activations = activation_model.predict(X[:10])\n",
    "    \n",
    "    # 绘制\n",
    "    fig, axes = plt.subplots(1, len(activations), figsize=(15, 4))\n",
    "    \n",
    "    for i, (activation, ax) in enumerate(zip(activations, axes)):\n",
    "        # 绘制热力图\n",
    "        im = ax.imshow(activation.T, aspect='auto', cmap='viridis')\n",
    "        ax.set_title(f'层 {i+1}')\n",
    "        ax.set_xlabel('样本')\n",
    "        ax.set_ylabel('神经元')\n",
    "        plt.colorbar(im, ax=ax)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# visualize_layer_activations(model, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "**神经网络的优缺点：**\n",
    "\n",
    "### 优点\n",
    "1. 强大的表示能力\n",
    "2. 自动特征提取\n",
    "3. 端到端学习\n",
    "\n",
    "### 缺点\n",
    "1. 需要大量数据\n",
    "2. 训练计算成本高\n",
    "3. 可解释性差（\"黑盒\"）\n",
    "4. 容易过拟合\n",
    "\n",
    "**实践技巧：**\n",
    "- 使用 ReLU 激活函数缓解梯度消失\n",
    "- 使用 Batch Normalization 稳定训练\n",
    "- 使用 Dropout 防止过拟合\n",
    "- 合理初始化权重（Xavier/He）\n",
    "- 使用合适的优化器（Adam、SGD+Momentum）"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
